---
title: "Trabalho Computacional 2"
subtitle: "EST077 - 2025.1"
author: "Milena Paz Freitas"
lang: pt-BR
format: html
table-of-contents: true
editor: source
self-contained: true
code-fold: show
code-summary: "<b>Mostrar/esconder código</b>"
---

# Seção 1:

Suponha que se tenha uma amostra aleatória $Y_1, . . . , Y_n$ de $Y\sim\text{Gama}(\alpha,\beta)$, com $\alpha$ e $\beta$ desconhecidos. O objetivo é encontrar os estimadores de máxima verossimilhança de $\alpha$ e $\beta$.

Gera-se parâmetros alfa e beta e respectiva amostra aleatoriamente para realizar simulação:

```{r}
#| code-fold: FALSE
n<- 100
alfa <- runif(1,max=10)
beta <- runif(1,max=10)

Y <- rgamma(n, shape=alfa, scale=beta)
```

OBS: Nesse caso, os parâmetros estarão limitados entre 0 e 10, mas essa informação não será relevante na realização do exercício.

## 1.1 Método A)

Esta abordagem envolve a maximização numérica da função log-verossimilhança $l(\alpha,\beta)$ quanto aos 2 argumentos, fazendo uso da função `stats::optim` no R.

Primeiro, devemos encontrar e definir $l(\alpha,\beta)$ como função no R:

$$
L(\alpha,\beta)=\prod^n_{i=1}f_Y(y_i)=\prod^n_{i=1}\frac{1}{\Gamma(\alpha)\beta^\alpha}y_i^{a-1}e^{-y_i/\beta}=
\frac{1}{\Gamma(\alpha)^n\beta^{n\alpha}}\left[\prod^n_{i=1}y_i\right]^{\alpha-1}e^{-\sum_iy_i/\beta}
$$

$$
\Rightarrow l(\alpha,\beta)=(\alpha-1)\ln\left[\prod^n_{i=1}y_i\right]-\frac{\sum^n_{i=1}y_i}{\beta}
-n\{\ln\Gamma(\alpha) + \alpha\ln(\beta)\}
$$

Implementando no R e maximizando:

```{r,warning=FALSE}
verossim <- function(param,y=Y){
  a <- param[1]
  b <- param[2]
  L <- log(prod(y))*(a-1) - sum(y)/b - n*log(gamma(a)*b^a)
  return(-L) #valor negativo para que a funcao seja maximizada e nao minimizada
}
#maximizacao
EMV <- optim(par=c(5,5),fn=verossim)$par
```

## 1.2 Método B)

Para realizar este método, divide-se a tarefa em duas etapas: Primeiro, deve-se encontrar o EMV de $\beta$, $\hat\beta$, assumindo que $\alpha$ é conhecido e substituir $\hat\beta$ em $l(\alpha,\beta)$. Assim, utilizando de métodos de maximização numéricos, encontra-se um $\hat\alpha$. Em seguida, basta obter $\hat\beta$ com base nesse resultado.

$$
\frac{d l(\beta|\alpha)}{d\beta}= -\frac{n\alpha}{\beta}+\frac{\sum_{i=1}^n y_i}{\beta^2}
$$

Solucionando para $\frac{d l(\beta|\alpha)}{d\beta}|^{\beta=\hat\beta}=0$, temos:

$$
\hat\beta = \frac{\sum_{i=1}^n y_i}{n\alpha}
$$

Substituindo em $l(\alpha,\beta)$:

$$
\begin{aligned}
l(\alpha,\hat\beta)&= 
(\alpha-1)\ln\left[\prod^n_{i=1}y_i\right]-\frac{\sum^n_{i=1}y_i}{\frac{\sum_{i=1}^n y_i}{n\alpha}}
-n\{\ln\Gamma(\alpha) + \alpha\ln\left(\frac{\sum_{i=1}^n y_i}{n\alpha}\right)\}
\\
&=(\alpha-1)\ln\left[\prod^n_{i=1}y_i\right]-n\alpha
-n\{\ln\Gamma(\alpha) + \alpha\ln\left(\frac{\bar y}{\alpha}\right)\}
\end{aligned}
$$

### i.  Maximização por gride de valores de $\alpha$:

```{r}
verossim.2 <- function(a,y=Y){
  log(prod(y))*(a-1) - n*a - n*log(gamma(a)*(mean(y)/a)^a)
}
a <- seq(0.01,100,1)
L <- verossim.2(a)
par(mfrow=c(2,3),mar=c(4.1,4.1,2,1))
plot(a,L,type="l")

# 5 repeticoes
for(i in 1:5){
  a <- seq(0.01,a[L==max(L)]+3/i,10^(-i))
  L <- verossim.2(a)
  plot(a, L,type="l", main=paste0(i,"a repetição"),
       xlab=expression(alpha))
  abline(v=a[L==max(L)],col="red",lty=2)
  mtext(bquote(hat(alpha) ==.(a[L==max(L)])),side=1,line=-1,
        adj=1,cex=.85)
}

```

Substituindo no estimador $\hat\beta$ obtido anteriormente:

```{r}
a <- a[L==max(L)]
b <- mean(Y)/a
```

### ii. Maximização por `stats::optim`:

```{r,warning=F}
a2 <- optim(5,
           fn=function(p) return(-verossim.2(p)))$par
b2 <- mean(Y)/a
```

## Comparando os resultados

Abaixo temos uma tabela comparando os valores obtidos pelo método A, B em comparação com os parâmetros gerados:

```{r}
#| code-fold: true
tabela <- rbind(c(alfa=alfa,beta=beta),
                EMV,
                c(a,b),
                c(a2,b2))
rownames(tabela) <- c("Parâmetros","Método A","Método B.1","Método B.2")
knitr::kable(tabela,col.names = c("$\\alpha$","$\\beta$"),escape=F)
```

Os resultados são bem semelhantes para ambos os métodos.

# Seção 2:

## 2.1 Densidade Cauchy

O objetivo é encontrar o EMV de $\theta$ de dada amostra de $X\sim\text{Cauchy}(\theta,1)$. Para estimar a locação da distribuição, pode-se usar o método de Newton-Rhapson para encontrar a raiz de $S(\hat\theta)=\frac{d l(\theta)}{d\theta}|^{\theta=\hat\theta}=0$ da seguinte forma:

1. Definir um $\hat\theta^{(0)}$ e um erro $\epsilon$.

2. Calcular $\hat\theta^{(n)}=\hat\theta^{(n-1)}+\frac{S(\hat\theta^{(n-1)})}{H(\hat\theta^{(n-1)})}$, 
onde $H(\theta)=-\frac{d^2 l(\theta)}{d\theta^2}$.

3. Se $|\hat\theta^{(n)}-\hat\theta^{(n-1)}|<\epsilon$ pare e tome $\hat\theta=\hat\theta^{(n)}$

4. Caso contrário, voltar ao passo 2.

Então, definamos $S(\theta)$ e $H(\theta)$ para uma a.a. de $X$:

$$
L(\theta)=\prod^n_{i=1}f_X(x_i)=\frac{1}{\pi^n}\prod^n_{i=1}\frac{1}{1+(x_i-\theta)^2}
$$

Como independe de de $\theta$, podemos desconsiderar o fator $\frac{1}{\pi^n}$ para a função log-verossimilhança:

$$
\begin{aligned}
& l(\theta)=-\sum^n_{i=1}\ln(1+(x_i-\theta)^2)
\\
& \Rightarrow S(\theta)=\sum^n_{i=1}\frac{2(x_i-\theta)}{1+(x_i-\theta)^2}
\\
& \Rightarrow H(\theta)=\sum^n_{i=1}\frac{2[1-(x_i-\theta)^2]}{[1+(x_i-\theta)^2]^2}
\end{aligned}
$$

Por fim, utiliza-se o algoritmo Newton-Raphson:

```{r}
set.seed(109) #semente para manter consistencia nos resultados
#por ser cauchy as vezes a aproximacao explode, ja que n defini um numero máximo de iterações
x <- rcauchy(100,location=10)

S <- function(t, X=x)
  2*sum((X-t)/(1+(X-t)^2))

H <- function(t,X=x)
  2*sum((1-(X-t)^2)/(1+(X-t)^2))^2

nH <- function(fun,deriv,x0,e){
  diff <- e+1
  iter <- numeric()
  iter[1] <- x0
  while(diff > e){
    derivada <- deriv(x0)
    if(derivada==0){
      cat("Ponto crítico encontrado\n"); break
    }
    prox <- x0 + fun(x0)/derivada
    diff <- abs(x0 - prox)
    x0 <- prox
    iter <- append(iter, x0)
  }
  return(list(res=x0,iter=iter))
}
tht0 <- 10.04490
obj <- nH(S,H,x0=tht0,e=1E-2)

L <- function(t,X=x)
  -log(prod(1+(x-t)^2))-n*log(pi)

tabela <- cbind(k=seq_along(obj$iter)-1,teta=obj$iter,L=sapply(obj$iter,L))

colunas <- c("k","$\\hat\\theta^{(k)}$","$\\ln L(\\hat\\theta^{(k)})$")

knitr::kable(tabela, col.names = colunas,row.names = NA)
```

## 2.2 Bolfarine e Sandoval, 2010

Seja $X$ uma v.a. com a seguinte fdp:

$$
f(x|\theta)=\frac{1}{2}(1+\theta x), \quad -1 \leq x \leq 1, \quad -1 \leq \theta \leq 1
$$

Dada uma a.a. $\pmb{X}=x_1,...,x_n$ de $X$, o objetivo é obter um estimador sobre $\theta$ usando o método Newton-Rhapson.

Implementando o conjunto de dados no livro de Bolfarine e Sandoval, de tamanho $n=20$ e com $\theta=0.4$

```{r}
x <- c(0.3374,0.9285,0.6802,-0.2139,0.1052,
        -0.9793,-0.2623,-0.1964,0.5234,-0.0349,
        -0.6082,0.7509,0.3424,-0.7010,-0.2605,
        0.4077,-0.7435,0.9862,0.9704,0.5313)
```

Achando a função de log-verossimilhança para $\theta$:

$$
\begin{aligned}
&L(\theta)=\prod^n_{i=1}\left(\frac{1+\theta x_i}{2}\right)
\\
& \Rightarrow l(\theta)=\sum^n_{i=1}\ln(1/2+\theta x_i/2)=-n\ln2+ \sum^n_{i=1} \ln(1+\theta x_i)
\end{aligned}
$$

Ignorando a parcela $-n\ln2$, pois independe de $\theta$, usaremos:

$$
\begin{aligned}
&l(\theta)=\sum^n_{i=1} \ln(1+\theta x_i)
\\
& \Rightarrow S(\theta)= \sum^n_{i=1}\frac{x_i}{1+\theta x_i}
\\
& \Rightarrow H(\theta)= - \sum^n_{i=1}\frac{-x_i^2}{(1+\theta x_i)^2}=\sum^n_{i=1}\frac{x_i^2}{(1+\theta x_i)^2}
\end{aligned}
$$

Com essas funções, usamos o algoritmo Newton-Raphson para obter $\hat\theta$, como feito anteriormente:

```{r}
S <- function(t, X=x)
  sum(x/(1+t*x))

H <- function(t,X=x)
  sum(x^2/(1+t*x)^2)

tht0 <- mean(x)
obj <- nH(S,H,x0=tht0,e=1E-10)

L <- function(t,X=x)
  log(prod((0.5+x*t/2)^2))

tabela <- cbind(k=seq_along(obj$iter)-1,teta=obj$iter,L=sapply(obj$iter,L))
knitr::kable(tabela, col.names = colunas,row.names = NA)
```
